{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Implement Cholesky Decomposition"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-30T01:52:17.578931Z","iopub.status.busy":"2023-11-30T01:52:17.578634Z","iopub.status.idle":"2023-11-30T01:52:17.591862Z","shell.execute_reply":"2023-11-30T01:52:17.590897Z","shell.execute_reply.started":"2023-11-30T01:52:17.578906Z"},"trusted":true},"outputs":[],"source":["%%writefile cholesky.py\n","import numpy as np\n","from scipy.linalg import cholesky as cho, cho_solve\n","\n","def backwardSubstituition(U:np.array, b:np.array) -> np.array:\n","    '''\n","    For upper triangular matrix\n","    '''\n","    N = U.shape[0]\n","    b_ = np.copy(b)\n","    b_[N - 1] /= U[N - 1, N - 1]\n","    for idx in range(N - 2, -1, -1):\n","        b_[idx] = (b_[idx] - (U[idx, idx + 1:N] @ b_[idx + 1:N])) / U[idx, idx]\n","    return b_\n","\n","def forwardSubstituition(L:np.array, b:np.array) -> np.array:\n","    '''\n","    For lower triangular matrix\n","    '''\n","    N = L.shape[0]\n","    b_ = np.copy(b)\n","    b_[0] /= L[0, 0]\n","    for idx in range(1, N):\n","        b_[idx] = (b_[idx] - (L[idx, :idx] @ b_[:idx])) / L[idx, idx]\n","    return b_\n","\n","\n","\n","def cholesky(A:np.array) -> np.array:\n","    A_ = np.copy(A)\n","    N = A_.shape[0]\n","    for idx in range(N):\n","        A_[idx:N, idx] -= (A_[idx:N, :idx] @ A_[idx, :idx].T)\n","        A_[idx:N, idx] /= -np.sqrt(-A_[idx, idx]) if A_[idx, idx] < 0 else np.sqrt(A_[idx, idx])\n","\n","    return np.tril(A_)\n","\n","def cholesky_solve(L:np.array, b:np.array) -> np.array:\n","    return backwardSubstituition(L.T, forwardSubstituition(L, b))"]},{"cell_type":"markdown","metadata":{},"source":["# Implement parameter wrapper classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.595228Z","iopub.status.busy":"2023-11-30T01:52:17.594896Z","iopub.status.idle":"2023-11-30T01:52:17.604997Z","shell.execute_reply":"2023-11-30T01:52:17.604246Z","shell.execute_reply.started":"2023-11-30T01:52:17.595197Z"},"trusted":true},"outputs":[],"source":["%%writefile paramsSpace.py\n","\n","from typing import Iterable\n","import numpy as np\n","\n","\n","\n","'''\n","Class to wrap Real, Integer, Categorical hyperparameter\n","'''\n","class Real:\n","    def __init__(self, low, high) -> None:\n","        self.__low = low\n","        self.__high = high\n","    \n","    def sample(self, shape):\n","        s = np.random.uniform(self.__low, self.__high, size=shape)\n","        return s\n","    \n","    def interval(self):\n","        return np.array([self.__low, self.__high])\n","    \n","class Int:\n","    def __init__(self, low, high) -> None:\n","        self.__low = low\n","        self.__high = high\n","\n","    def sample(self, shape):\n","        s = np.random.randint(self.__low, self.__high + 1, size=shape)\n","        return s\n","    \n","    def interval(self):\n","        return np.array([self.__low, self.__high])\n","\n","class Cat:\n","    def __init__(self, category:Iterable) -> None:\n","        self.category = category\n","\n","    def sample(self, shape):\n","        s = np.random.randint(0, len(self.category), size=shape)\n","        return s\n","    \n","    def interval(self):\n","        return np.array([0, len(self.category) - 1])\n","\n","class Choice:\n","    def __init__(self, choice:Iterable) -> None:\n","        self.choice = choice\n","\n","    def sample(self, shape):\n","        s = np.random.choice(self.choice, size=shape, replace=True)\n","        return s\n","    \n","    def interval(self):\n","        return np.array([0, len(self.choice) - 1])\n","\n","class RealKernels:\n","    '''\n","    Class to wrap hyperparameter for Gaussian Process Regressor kernel\n","    '''\n","    def __init__(self, bounds:np.array) -> None:\n","        self.__bounds = bounds\n","\n","    def sample(self, shape):\n","        s = np.random.uniform(self.__bounds[:, 0], self.__bounds[:, 1], size=shape)\n","        return s\n","\n","    def interval(self):\n","        return self.__bounds\n","\n","class AcqParams:\n","    '''\n","    Class to wrap hyperparameter so that \n","    it can be passed into acquisition function\n","    '''\n","    def __init__(self, params) -> None:\n","        self.__params = params\n","\n","    def sample(self, shape):\n","        s = np.zeros(shape)\n","        for idx, p in enumerate(self.__params):\n","            s[:, idx] = self.__params[p].sample(shape[0])\n","        return s\n","\n","    def interval(self):\n","        bounds = np.zeros((len(self.__params), 2))\n","        for idx, p in enumerate(self.__params):\n","            bounds[idx] = self.__params[p].interval()\n","        return bounds\n","    \n","    def Int_idxs(self):\n","        mask_int = []\n","        for idx, p in enumerate(self.__params):\n","            if type(self.__params[p]) == Int: \n","                mask_int.append(idx)\n","        mask_int = np.array(mask_int)\n","        return mask_int\n","    \n","    def Cat_idxs(self):\n","        mask_cat = []\n","        for idx, p in enumerate(self.__params):\n","            if type(self.__params[p]) == Cat: \n","                mask_cat.append(idx)\n","        mask_cat = np.array(mask_cat)\n","        return mask_cat\n","\n","\n","\n","\n","def convert_to_bound(params:dict) -> np.array:\n","    '''\n","    Convert hyperparameter dictionary into numpy array bounds\n","    '''\n","    if \"theta\" in params:\n","        bound = params[\"theta\"].interval()\n","    elif \"x\" in params:\n","        bound = params[\"x\"].interval()\n","    else:\n","        bound = np.zeros(shape=(len(params), 2))\n","        for idx, p in enumerate(params):\n","            bound[idx] = params[p].interval()\n","    return bound\n","\n","def decode_acq_params(encoded, params):\n","    '''\n","    Decode the encoded acquisition data back to \n","    hyperparameter dictionary\n","    '''\n","    params_ = {}\n","    for idx, p in enumerate(params):\n","        if type(params[p]) == Int:\n","            params_[p] = encoded[\"x\"][idx].astype(int)\n","        elif type(params[p]) == Cat:\n","            params_[p] = params[p].category[encoded[\"x\"][idx].astype(int)]\n","        elif type(params[p]) == Real:\n","            params_[p] = encoded[\"x\"][idx]\n","    return params_\n","\n","def convert_to_params(encoded, params):\n","    '''\n","    Convert the numpy array (encoded hyperparameter) to\n","    hyperparameter dictionary\n","    '''\n","    params_ = {}\n","    # print(encoded)\n","    if \"theta\" in params:\n","        params_[\"theta\"] = encoded\n","        return params_\n","    elif \"x\" in params:\n","        params_[\"x\"] = encoded\n","        return params_\n","    for idx, p in enumerate(params):\n","        if type(params[p]) == Real:\n","            params_[p] = encoded[idx]\n","        elif type(params[p]) == Int:\n","            params_[p] = encoded[idx].astype(int)\n","        elif type(params[p]) == Cat:\n","            params_[p] = params[p].category[encoded[idx].astype(int)]\n","    return params_\n","            \n","    \n","\n","if __name__ == \"__main__\":\n","    from keras.layers import Dense, Conv2D\n","    params = {\n","        \"Layer1\":Cat([Dense, Conv2D]),\n","        \"Layer1_filters\":Cat([8, 16, 32]),\n","        \"Layer1_kernel_size\":Cat([3, 5, 7]),\n","        \"Layer1_units\":Int(12, 64),\n","        \"Layer1_activation\":Cat([\"relu\", \"sigmoid\"]),\n","\n","        \"Layer2_units\":Int(32, 128),\n","        \"Layer2_activation\":Cat([\"relu\", \"sigmoid\"]),\n","        \"optimizer\":Cat([\"adam\", \"sgd\", \"rmsprop\"]),\n","        \"epochs\":Int(5, 20),\n","        \"batch_size\":Int(64, 256),\n","        \"learning_rate\":Real(0.0001, 0.01)\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.710495Z","iopub.status.busy":"2023-11-30T01:52:17.709752Z","iopub.status.idle":"2023-11-30T01:52:17.72017Z","shell.execute_reply":"2023-11-30T01:52:17.719117Z","shell.execute_reply.started":"2023-11-30T01:52:17.710466Z"},"trusted":true},"outputs":[],"source":["%%writefile util.py\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# import seaborn as sns\n","import pandas as pd\n","from paramsSpace import convert_to_params\n","\n","def plot1D(x, f, opt, n_samples, label):\n","    X = x.reshape(-1, 1)\n","    y = [f(x_) for x_ in x]\n","    if hasattr(opt, \"_gp\"):\n","        model = opt._gp\n","    elif hasattr(opt, \"gp\"):\n","        model = opt.gp\n","\n","    if hasattr(opt, \"x_history\") and hasattr(opt, \"y_history\"):\n","        plt.scatter(opt.x_history, opt.y_history, c=\"blue\", s=100, marker=\"+\")\n","\n","    mean, std = model.predict(X, return_std=True)\n","\n","    y_samples = model.sample_y(X, n_samples)\n","    for idx, single_prior in enumerate(y_samples.T):\n","        # print(\"Single Prior : \", single_prior)\n","        plt.plot(\n","            x,\n","            single_prior.reshape(x.shape[0]),\n","            linestyle=\"--\",\n","            alpha=0.7,\n","            label=f\"Sampled function #{idx + 1}\",\n","        )\n","    plt.plot(x, mean, label=label)\n","    plt.plot(x, y, label=\"Actual\")\n","    plt.fill_between(\n","        x,\n","        mean.reshape(x.shape[0], ) - std,\n","        mean.reshape(x.shape[0], ) + std,\n","        alpha=0.1,\n","        label=fr\"{label} mean $\\pm$ 1 std. dev.\",\n","    )\n","    if hasattr(opt, \"x_history\") and hasattr(opt, \"y_history\"):\n","        plt.scatter(opt.x_history, opt.y_history, c=\"blue\", s=100, marker=\"+\")\n","\n","    plt.legend()\n","    # plt.show()\n","\n","def plot2D(x, y, f, opt):\n","    x_, y_ = np.meshgrid(x, y)\n","    # z = f(x_, y_)\n","    z = np.zeros_like(x_)\n","    for idx in range(x_.shape[0]):\n","        for jdx in range(x_.shape[1]):\n","            z[idx, jdx] = f(x_[idx][jdx], y_[idx][jdx])\n","    print(z.max())\n","    fig, axs = plt.subplots(ncols=3)\n","    \n","    c_actual = axs[0].contourf(x_, y_, z, levels=100, cmap=\"viridis\")\n","    axs[0].set_title(\"Actual\")\n","    axs[0].grid()\n","    axs[0].plot()\n","\n","\n","    input_mesh = np.column_stack((x_.ravel(), y_.ravel()))\n","    mean, std = opt.gp.predict(input_mesh, return_std=True)\n","\n","    z_predict = mean.reshape(x_.shape)\n","    c_predict = axs[1].contourf(x_, y_, z_predict, levels=100, cmap=\"viridis\")\n","    axs[1].set_title(\"Predict\")\n","    axs[1].grid()\n","    axs[1].plot()\n","\n","    if hasattr(opt, \"x_history\") and hasattr(opt, \"y_history\"):\n","        axs[1].scatter(opt.x_history[:opt.init, 0], opt.x_history[:opt.init, 1], c=\"black\", s=75, marker=\"+\")\n","        axs[1].scatter(opt.x_history[opt.init:, 0], opt.x_history[opt.init:, 1], c=\"blue\", s=75, marker=\"+\")\n","\n","\n","    diff = np.abs(z - z_predict)\n","    contour_diff = axs[2].contourf(x_, y_, diff, levels=100, cmap='coolwarm')\n","    axs[2].set_title(\"Actual - Predicted\")\n","    axs[2].grid()\n","    cbar_diff = fig.colorbar(contour_diff, ax=axs[2], orientation='vertical')\n","    # cbar_diff.set_label('')\n","\n","    fig.colorbar(c_actual, ax=[axs[0], axs[1]], orientation=\"horizontal\") \n","    plt.show()\n","\n","def kdeplot(y_true, y_pred):\n","    plt.figure(figsize=(24, 8))\n","    ax1 = sns.kdeplot(data=y_true, color=\"r\", label=\"Actual\")\n","    sns.kdeplot(data=y_pred, color=\"b\", label=\"Predicted\")\n","    ax1.set_title(\"Predicted VS Actual\")\n","    plt.legend()\n","    plt.show()\n","\n","def regplot(y_true, y_pred):\n","    plt.figure(figsize=(40, 25))\n","    ax = sns.regplot(x=y_true, y=y_pred)\n","    plt.title(\"Best Fit Line\")\n","    ax.set_xlabel(\"Actual\")\n","    ax.set_ylabel(\"Predicted\")\n","    plt.show()\n","\n","\n","\n","from matplotlib.animation import FuncAnimation\n","\n","class Result:\n","    def __init__(self, algo_name, func_name, best_sol, \n","                best_sol_decoded, best_fitness, params,\n","                fitness_history, pop_history) -> None:\n","        self.__algo_name = algo_name\n","        self.__func_namae = func_name\n","        self.x = best_sol\n","        self.x_decoded = best_sol_decoded\n","        self.fun = best_fitness\n","        self.__fitness_history = fitness_history\n","        self.__pop_history = pop_history\n","        self.__params = params\n","\n","    def res(self) -> pd.DataFrame:\n","\n","        # if n_iter is None:\n","        #     pop = self.__pop_history[-1]\n","        #     fitness = self.__fitness_history[-1]\n","        # else:\n","        #     pop = self.__pop_history[n_iter]\n","        #     fitness = self.__fitness_history[n_iter]\n","        n_iter = self.__pop_history.shape[0]\n","        pop_decoded = []\n","        for idx in range(n_iter):\n","            for p in self.__pop_history[idx]:\n","                pop_decoded.append(convert_to_params(p, self.__params))\n","        \n","        fitness = self.__fitness_history.flatten()\n","        # print(n_iter)\n","        # print(pop_decoded)\n","        # print(fitness)\n","        # print({k:[p[k] for p in pop_decoded] for k in self.__params})\n","        # print(np.repeat(np.arange(0, n_iter + 1), self.__pop_history.shape[1]))\n","        return pd.DataFrame(\n","            {\n","                **{k:[p[k] for p in pop_decoded] for k in self.__params},\n","                \"fitness\":fitness,\n","                \"iteration\":np.repeat(np.arange(0, n_iter), self.__pop_history.shape[1])\n","            }\n","        )\n","\n","    def func_name(self):\n","        return self.__func_namae\n","    \n","    def algo_name(self):\n","        return self.__algo_name\n","\n","    def fitness_history(self):\n","        return self.__fitness_history\n","    \n","    def population_history(self):\n","        return self.__pop_history\n","    \n","    def plot_fitness(self):\n","        fitness = np.min(self.__fitness_history, axis=1)\n","        plt.scatter([x for x in range(self.__fitness_history.shape[0])], fitness, marker=\"+\", c=\"black\", s=50)\n","        plt.plot([x for x in range(self.__fitness_history.shape[0])], fitness)\n","        plt.title(f\"Fitness Function : {self.__func_namae}\\nAlgorithm : {self.__algo_name}\")\n","        plt.xlabel(\"Iteration\")\n","        plt.ylabel(\"Fitness\")\n","        plt.grid()\n","        plt.show()\n","\n","    def plot_fitness_mean_bar(self):\n","        # for idx in range((self.__fitness_history.shape[0] )):\n","        #     plt.scatter([x for x in range(self.__fitness_history.shape[1])], self.__fitness_history[idx], marker=\"+\")\n","        mean = self.__fitness_history.mean(axis=1)\n","        plt.bar(x=[x for x in range(self.__fitness_history.shape[0])], height=mean)\n","        plt.title(\"Fitness Mean\")\n","        plt.style.use(\"seaborn-darkgrid\")\n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Implement Acquisition Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.722105Z","iopub.status.busy":"2023-11-30T01:52:17.721749Z","iopub.status.idle":"2023-11-30T01:52:17.733444Z","shell.execute_reply":"2023-11-30T01:52:17.732389Z","shell.execute_reply.started":"2023-11-30T01:52:17.722066Z"},"trusted":true},"outputs":[],"source":["%%writefile acquisition.py\n","\n","from scipy.stats import norm\n","import numpy as np\n","\n","class AcqFunc:\n","\n","    def __init__(self, gp) -> None:\n","        self.__gp = gp\n","\n","    def func(self, X, kind:[\"ucb\", \"lcb\", \"pi\", \"ei\", \"mix\"], **acq_params) -> callable:\n","\n","        if kind == \"ucb\":\n","            return self.GP_UCB(X, **acq_params)\n","        elif kind == \"lcb\":\n","            return self.GP_LCB(X, **acq_params)\n","        elif kind == \"ei\":\n","            return self.GP_EI(X, **acq_params)\n","        elif kind == \"pi\":\n","            return self.GP_PI(X, **acq_params)\n","        elif kind == \"mix\":\n","            pi = self.GP_PI(X, **acq_params)\n","            ei = self.GP_EI(X, **acq_params)\n","            ucb = self.GP_UCB(X, **acq_params)\n","            sum = pi + ei + ucb\n","            return (pi * pi / sum) + (ei * ei / sum) + (ucb * ucb / sum)\n","        \n","    def GP_EI(self, X, xi=0.01, kappa=0, y_opt:float=0):\n","        # print(y_opt)\n","        mean, std = self.__gp.predict(X, return_std=True)\n","        values = np.zeros_like(mean)\n","        mask = std > 0\n","        improve = y_opt - xi - mean[mask]\n","        scaled = improve / std[mask]\n","        cdf = norm.cdf(scaled)\n","        pdf = norm.pdf(scaled)\n","        exploit = improve * cdf\n","        explore = std[mask] * pdf\n","        values[mask] = exploit + explore\n","        return values\n","\n","    def GP_PI(self, X, xi=0.01, kappa=0, y_opt:float=0):\n","        mean, std = self.__gp.predict(X, return_std=True)\n","        values = np.zeros_like(mean)\n","        mask = std > 0\n","        improve = y_opt - xi - mean[mask]\n","        scaled = improve / std[mask]\n","        values[mask] = norm.cdf(scaled)\n","        return values\n","\n","    def GP_LCB(self, X, xi=0, kappa=1.96, y_opt=0):\n","        mean, std = self.__gp.predict(X, return_std=True)\n","        return mean - kappa * std\n","\n","    def GP_UCB(self, X, xi=0, kappa=1.96, y_opt=0):\n","        # print(\"X : \", X)\n","        mean, std = self.__gp.predict(X, return_std=True)\n","        return mean + kappa * std"]},{"cell_type":"markdown","metadata":{},"source":["# Implement optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.735878Z","iopub.status.busy":"2023-11-30T01:52:17.735211Z","iopub.status.idle":"2023-11-30T01:52:17.751112Z","shell.execute_reply":"2023-11-30T01:52:17.750228Z","shell.execute_reply.started":"2023-11-30T01:52:17.735853Z"},"trusted":true},"outputs":[],"source":["%% writefile optimizer.py\n","\n","import numpy as np\n","from abc import ABC, abstractmethod\n","from tqdm import tqdm\n","from util import Result\n","from paramsSpace import Real, Int, Cat, convert_to_bound, convert_to_params\n","import sys\n","\n","class Optimizer(ABC):\n","    '''\n","    Abstract optimizer class\n","    '''\n","    def __init__(self, func, params,\n","                popsize, n_iter, ttl,\n","                verbose, random_state\n","                ) -> None:\n","        \n","        self.func = func\n","        self.params = params\n","        self.popsize = popsize\n","        self.n_iter = n_iter\n","        self.verbose = verbose\n","        self.ttl = ttl\n","\n","        self.best_score = None\n","        self.best_params = None\n","        self.pbounds = None\n","        self.dimensions = len(self.params) if self.params is not None else None\n","\n","        # Seed the numpy first\n","        self.verify_random_state(random_state)\n","        if self.func is None and self.params is None:\n","            self.init_pop_fitness()\n","\n","\n","    @abstractmethod\n","    def optim(self):\n","        pass\n","    \n","    def init_dimension(self):\n","        '''\n","        Get the dimensions of the search space\n","        '''\n","        if \"theta\" in self.params:\n","            self.dimensions = self.params[\"theta\"].interval().shape[0]\n","        elif \"x\" in self.params:\n","            self.dimensions = self.params[\"x\"].interval().shape[0]\n","        else:\n","            self.dimensions = len(self.params)\n","\n","    def preprocess(self):\n","        '''\n","        Record the indices of Int and Cat types in the search space\n","        '''\n","        if \"theta\" in self.params:\n","            mask_int = np.array([])\n","            mask_cat = np.array([])\n","        elif \"x\" in self.params:\n","            mask_int = self.params[\"x\"].Int_idxs()\n","            mask_cat = self.params[\"x\"].Cat_idxs()\n","        else:\n","            mask_int = []\n","            mask_cat = []\n","            for idx, p in enumerate(self.params):\n","                if type(self.params[p]) == Int: \n","                    mask_int.append(idx)\n","                elif type(self.params[p]) == Cat:\n","                    mask_cat.append(idx)\n","            mask_int = np.array(mask_int)\n","            mask_cat = np.array(mask_cat)\n","        return mask_int, mask_cat\n","\n","    def init_pop_fitness(self):\n","        '''\n","        Initialize the population and calculate the fitness of individual\n","        '''\n","        self.pop = np.zeros(shape=(self.popsize, self.dimensions))\n","        if \"theta\" in self.params:\n","            self.pop = self.params[\"theta\"].sample((self.popsize, self.dimensions))\n","        elif \"x\" in self.params:\n","            self.pop = self.params[\"x\"].sample((self.popsize, self.dimensions))\n","        else:\n","            for idx, p in enumerate(self.params):\n","                self.pop[:, idx] = self.params[p].sample(shape=self.popsize)\n","        self.fitness = np.zeros((self.popsize, ))\n","        iteration = range(self.popsize)\n","\n","        if self.verbose:\n","            iteration = tqdm(iteration, desc=f\"Initializing {self.__class__.__name__}\")\n","        for idx, ind in zip(iteration, self.pop):      \n","            f = self.func(**convert_to_params(ind, self.params))\n","            self.fitness[idx] = f\n","\n","\n","    def set_func(self, func:callable):\n","        '''\n","        Set the function after the instance was initialized\n","        '''\n","        self.func = func\n","\n","    def set_params(self, params:dict):\n","        '''\n","        Set the search space after the instance was initialized\n","        '''\n","        self.params = params\n","\n","\n","    def verify_func_params(self):\n","        '''\n","        Verify that the target function and parameters range are provided\n","        '''\n","        if self.func is None:\n","            ValueError(\"Please provide your function\")\n","        elif self.params is None:\n","            ValueError(\"Please provide your parameters\")\n","\n","    def verify_random_state(self, random_state):\n","        if isinstance(random_state, int):\n","            return np.random.seed(random_state)\n","        elif random_state is None:\n","            return np.random.seed()\n","        \n","class DifferentialEvol(Optimizer):\n","    def __init__(self, func:callable=None, params:dict=None, mut_1=0.9, mut_2=0.9, \n","                crossp=0.95, popsize=10, ttl=np.inf,\n","                n_iter=20, verbose=0, random_state=None) -> None:\n","        super().__init__(func, params, popsize, n_iter, ttl, verbose, random_state)\n","        self.mut_1 = mut_1\n","        self.mut_2 = mut_2\n","        self.crossp = crossp\n","\n","    def optim(self) -> Result:\n","        self.verify_func_params()\n","        self.init_dimension()\n","        self.init_pop_fitness()\n","\n","        assert(self.popsize >= 3)\n","        \n","        age = np.ones((self.popsize, )) * np.inf\n","        if self.ttl > 0:\n","            age = np.ones((self.popsize, )) * self.ttl\n","            \n","        \n","        mask_int, mask_cat = self.preprocess()\n","\n","        self.pbounds = convert_to_bound(self.params)\n","        min_b, max_b = self.pbounds.T\n","\n","        pop_history = np.zeros((self.n_iter + 1, self.popsize, self.dimensions))\n","        fitness_history = np.zeros((self.n_iter + 1, self.popsize))\n","        pop_history[0] = self.pop.copy()\n","        fitness_history[0] = self.fitness.copy()\n","\n","        best_idx = np.argmax(self.fitness)\n","        best_x = self.pop[best_idx]\n","        self.best_score = self.fitness[best_idx]\n","\n","        for idx in range(self.n_iter):\n","            iteration = range(self.popsize)\n","            if self.verbose:\n","                iteration = tqdm(iteration, file=sys.stdout)\n","\n","            for jdx in iteration:\n","                age[jdx] -= 1\n","                if self.verbose:\n","                    iteration.set_description_str(desc=f\"Differential Evol {idx + 1}\")\n","                    iteration.set_postfix_str(f\"best_f : {round(self.best_score, 5)}\")\n","                idxs = [kdx for kdx in range(self.popsize) if kdx != jdx]\n","\n","                # Mutation strategy\n","                a, b, c = self.pop[np.random.choice(idxs, 3, replace = False)]\n","                mutant = a + self.mut_1 * (b - c) + self.mut_2 * (best_x - a)\n","\n","                # Boundary checking\n","                mutant[mutant > max_b] = max_b[mutant > max_b]\n","                mutant[mutant < min_b] = min_b[mutant < min_b]\n","\n","                # Cross over\n","                cross_points = np.random.rand(self.dimensions) < self.crossp\n","                if not np.any(cross_points):\n","                    cross_points[np.random.randint(0, self.dimensions)] = True\n","\n","                trial = np.where(cross_points, mutant, self.pop[jdx])\n","                if mask_int.size:\n","                    trial[mask_int] = np.round(trial[mask_int])\n","                if mask_cat.size:\n","                    trial[mask_cat] = np.round(trial[mask_cat])\n","                \n","                trial_ = convert_to_params(trial, self.params)\n","                f = self.func(**trial_)\n","\n","                if f > self.fitness[jdx] or (age[jdx] == -1):\n","                    self.fitness[jdx] = f\n","                    self.pop[jdx] = trial\n","                    age[jdx] = self.ttl\n","                    if f > self.best_score:\n","                        best_x = trial\n","                        self.best_score = f\n","\n","            fitness_history[idx + 1] = self.fitness.copy()\n","            pop_history[idx + 1] = self.pop.copy()\n","        self.best_params = convert_to_params(best_x, self.params)\n","\n","        return Result(self.__class__.__name__, self.func.__name__, \n","                    best_x, self.best_params, self.best_score, \n","                    self.params, fitness_history, pop_history)\n","\n","class HarmonySearch(Optimizer):\n","    def __init__(self, func:callable=None, params:dict=None, HMCR=0.7, PAR=0.3, BW:np.array=None, \n","                popsize=10, n_iter=20, ttl=np.inf, verbose=0, random_state=None) -> None:\n","        super().__init__(func, params, popsize, n_iter, ttl, verbose, random_state)\n","        self.HMCR = HMCR\n","        self.PAR = PAR\n","        self.BW = BW\n","    \n","    def optim(self):\n","        self.verify_func_params()\n","        self.init_dimension()\n","        self.init_pop_fitness()\n","        \n","        age = np.ones((self.popsize, )) * np.inf\n","        if self.ttl > 0:\n","            age = np.ones((self.popsize, )) * self.ttl\n","        \n","        mask_int, mask_cat = self.preprocess()\n","\n","        harmony_history = np.zeros((self.n_iter + 1, self.popsize, self.dimensions))\n","        fitness_history = np.zeros((self.n_iter + 1,self. popsize))\n","\n","        self.pbounds = convert_to_bound(self.params)\n","        min_b, max_b = self.pbounds.T\n","        diff = max_b - min_b\n","        if self.BW is not None:\n","            assert(self.BW.shape[0] == self.dimensions)\n","            diff = self.BW\n","\n","        harmony_history[0] = self.pop.copy()\n","        fitness_history[0] = self.fitness.copy()\n","\n","        best_idx = np.argmax(self.fitness)\n","        worst_idx = np.argmin(self.fitness)\n","        best_harmony = self.pop[best_idx]\n","        self.best_score = self.fitness[best_idx]\n","\n","        for idx in range(self.n_iter):\n","            r1_ = np.random.rand(self.popsize, self.dimensions)\n","            r2_ = np.random.rand(self.popsize, self.dimensions)\n","            r3_ = np.random.uniform(low=-1, high=1.001, size=(self.popsize, self.dimensions))\n","            iteration = range(self.popsize)\n","            if self.verbose:\n","                iteration = tqdm(iteration, file=sys.stdout)\n","\n","            for jdx in iteration:\n","                age[jdx] -= 1\n","                if self.verbose:\n","                    iteration.set_description_str(desc=f\"Harmony Search {idx + 1}\")\n","                    iteration.set_postfix_str(f\"best_f : {round(self.best_score, 5)}\")\n","                trial = np.zeros(self.dimensions)\n","                for kdx in range(self.dimensions):\n","                    if r1_[jdx][kdx] < self.HMCR:\n","                        trial[kdx] = self.pop[np.random.randint(0, self.popsize)][kdx]\n","                    else:\n","                        trial[kdx] = np.random.uniform(min_b[kdx], max_b[kdx] + 0.001)\n","                    if r2_[jdx][kdx] < self.PAR:\n","                        trial[kdx] += r3_[jdx][kdx] * diff[kdx]\n","                trial[trial > max_b] = max_b[trial > max_b]\n","                trial[trial < min_b] = min_b[trial < min_b]\n","                if mask_int.size:\n","                    trial[mask_int] = np.round(trial[mask_int])\n","                if mask_cat.size:\n","                    trial[mask_cat] = np.round(trial[mask_cat])\n","\n","                trial_ = convert_to_params(trial, self.params)\n","                f = self.func(**trial_)\n","\n","                # Update worst_idx first and followed by best_idx\n","                # Ignore the trial harmony if worse than the worst harmony\n","                if f > self.fitness[worst_idx] or (age[jdx] == -1):\n","                    self.pop[worst_idx] = trial\n","                    self.fitness[worst_idx] = f\n","                    age[jdx] = self.ttl\n","                    if f > self.best_score:\n","                        best_harmony = trial\n","                        self.best_score = f\n","                    worst_idx = np.argmin(self.fitness)\n","\n","            self.best_params = convert_to_params(best_harmony, self.params)\n","            fitness_history[idx + 1] = self.fitness.copy()\n","            harmony_history[idx + 1] = self.pop.copy()\n","\n","        return Result(self.__class__.__name__, self.func.__name__, \n","                    best_harmony, self.best_params, self.best_score, \n","                    self.params, fitness_history, harmony_history)\n","\n","class ParticleSwarm(Optimizer):\n","    def __init__(self, func:callable=None, params:dict=None, inertia=.5, cognitive=1.5, social=1.5, \n","                popsize=10, n_iter=20, ttl=np.inf,\n","                verbose=0, random_state=None) -> None:\n","        super().__init__(func, params, popsize, n_iter, ttl, verbose, random_state)\n","        self.inertia = inertia\n","        self.cognitive = cognitive\n","        self.social = social\n","\n","    def optim(self) -> Result:\n","\n","        self.verify_func_params()\n","        self.init_dimension()\n","        self.init_pop_fitness()\n","        \n","        age = np.ones((self.popsize, )) * np.inf\n","        if self.ttl > 0:\n","            age = np.ones((self.popsize, )) * self.ttl\n","        \n","        mask_int, mask_cat = self.preprocess()\n","\n","        swarm_history = np.zeros((self.n_iter + 1, self.popsize, self.dimensions))\n","        fitness_history = np.zeros((self.n_iter + 1, self.popsize))\n","\n","        self.pbounds = convert_to_bound(self.params)\n","        min_b, max_b = self.pbounds.T\n","        diff = np.fabs(min_b - max_b)\n","\n","        swarm_history[0] = self.pop.copy()\n","        fitness_history[0] = self.fitness.copy()\n","        velocity = np.random.uniform(-np.abs(diff), np.abs(diff), (self.popsize, self.dimensions))\n","        swarm_best = self.pop.copy()\n","\n","        best_idx = np.argmax(self.fitness)\n","        best_position = swarm_best[best_idx]\n","        self.best_score = self.fitness[best_idx]\n","\n","        for idx in range(self.n_iter):\n","            r_p = np.random.rand(self.popsize, self.dimensions)\n","            r_s = np.random.rand(self.popsize, self.dimensions)\n","            iteration = range(self.popsize)\n","            if self.verbose:\n","                iteration = tqdm(iteration, file=sys.stdout)\n","\n","            for jdx in iteration:\n","                age[jdx] -= 1\n","                if self.verbose:\n","                    iteration.set_description_str(f\"Swarm Particle {idx + 1}\")\n","                    iteration.set_postfix_str(f\"best_f : {round(self.best_score, 5)}\")\n","                velocity[jdx, :] = self.inertia * velocity[jdx, :] + self.cognitive * r_p[jdx, :] * (swarm_best[jdx, :] - self.pop[jdx, :]) + \\\n","                                            self.social * r_s[jdx, :] * (best_position - self.pop[jdx, :])\n","                self.pop[jdx, :] += velocity[jdx, :]\n","\n","                self.pop[jdx, self.pop[jdx, :] > max_b] = max_b[self.pop[jdx, :] > max_b]\n","                self.pop[jdx, self.pop[jdx, :] < min_b] = min_b[self.pop[jdx, :] < min_b]\n","                if mask_int.size:\n","                    self.pop[jdx][mask_int] = np.round(self.pop[jdx][mask_int])\n","                if mask_cat.size:\n","                    self.pop[jdx][mask_cat] = np.round(self.pop[jdx][mask_cat])\n","                trial_ = convert_to_params(self.pop[jdx], self.params)\n","                \n","                f = self.func(**trial_)\n","                if f > self.fitness[jdx] or (age[jdx] == -1):\n","                    swarm_best[jdx] = self.pop[jdx]\n","                    self.fitness[jdx] = f\n","                    age[jdx] = self.ttl\n","                    if f > self.best_score:\n","                        best_position = swarm_best[jdx]\n","                        self.best_score = f\n","\n","            fitness_history[idx + 1] = self.fitness.copy()\n","            swarm_history[idx + 1] = self.pop.copy()\n","\n","\n","        self.best_params = convert_to_params(best_position, self.params)\n","\n","        return Result(self.__class__.__name__, self.func.__name__, \n","                    best_position, self.best_params, self.best_score, \n","                    self.params, fitness_history, swarm_history)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Implement Gaussian Process Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-11-30T01:52:17.825117Z","iopub.status.busy":"2023-11-30T01:52:17.824856Z","iopub.status.idle":"2023-11-30T01:52:17.835767Z","shell.execute_reply":"2023-11-30T01:52:17.834908Z","shell.execute_reply.started":"2023-11-30T01:52:17.825095Z"},"trusted":true},"outputs":[],"source":["%%writefile gaussRgr.py\n","\n","from cholesky import cholesky, cholesky_solve, forwardSubstituition\n","import numpy as np\n","from scipy.linalg import cholesky as cho, cho_solve, solve_triangular\n","import sklearn.gaussian_process.kernels as kernels\n","import scipy.optimize\n","from operator import itemgetter\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score\n","import optimizer\n","from paramsSpace import RealKernels\n","\n","class GPR:\n","    def __init__(\n","        self,          \n","        kernel=None, \n","        epsilon=1e-10,\n","        random_state=None,\n","        n_restarts_optimizer=0,\n","        normalize_y=False,\n","        optimizer=\"de\"\n","        ) -> None:\n","        self.y_mean: np.array\n","        self.epsilon: np.array\n","        self.alpha: np.array\n","        self.L: np.array\n","        self.kernel_ = kernel\n","        self.epsilon = epsilon\n","        self.random_state = random_state\n","        self.X_train, self.y_train = None, None\n","        self.n_restarts_optimizer= n_restarts_optimizer\n","        self.normalize_y = normalize_y\n","        self.optimizer=optimizer\n","\n","    def fit(self, X, y):\n","        if self.random_state is None or self.random_state is np.random:\n","            self.random_state = np.random.mtrand._rand\n","        if isinstance(self.random_state, int):\n","            self.random_state = np.random.RandomState(self.random_state)\n","\n","        if self.kernel_ is None:\n","            self.kernel = kernels.ConstantKernel(1.0, constant_value_bounds=\"fixed\") * kernels.RBF(\n","                1.0, length_scale_bounds=\"fixed\"\n","            )\n","        else:\n","            # Use self.kernel so that we get the unfitted kernel not keep \n","            # recloning the fitted one since self.kernel_ holds the \n","            # original unfitted kernel\n","            self.kernel = kernels.clone(self.kernel_)\n","\n","        if self.normalize_y:\n","            scaler = StandardScaler()\n","            y = scaler.fit_transform(y)\n","            self._y_train_mean = scaler.mean_\n","            self._y_train_std = scaler.scale_\n","\n","        else:\n","            self._y_train_mean = np.zeros(1)\n","            self._y_train_std = 1\n","\n","        self.X_train = np.copy(X)\n","        self.y_train = np.copy(y)\n","\n","\n","        if self.kernel.n_dims > 0:\n","            def kernelFunc(theta, evaluate_grad=True if self.optimizer==\"l-bfgs-b\" else False):\n","                if evaluate_grad:\n","                    lml, grad = self.log_marginal_likelihood(theta, evaluate_grad)\n","                    return -lml, -grad\n","                else:\n","                    lml = self.log_marginal_likelihood(theta, evaluate_grad)\n","                    return lml\n","            if self.optimizer == \"l-bfgs-b\":\n","                optima = [\n","                    (\n","                        self.optimization(\n","                            kernelFunc, self.kernel.theta, self.kernel.bounds, self.optimizer\n","                        )\n","                    )\n","                ]\n","                if self.n_restarts_optimizer > 0:\n","                    bounds = self.kernel.bounds\n","                    for iteration in range(self.n_restarts_optimizer):\n","                        theta_initial = self.random_state.uniform(bounds[:, 0], bounds[:, 1])\n","                        optima.append(\n","                            self.optimization(kernelFunc, theta_initial, bounds, self.optimizer)\n","                        )\n","\n","                # Get the log-marginal-likelihood value\n","                lml_values = list(map(itemgetter(1), optima))\n","                # Get the kernel theta with the smallest negative log-marginal-likelihood value(which means largest)\n","                self.kernel.theta = optima[np.argmin(lml_values)][0]\n","                self.kernel._check_bounds_params()\n","            elif self.optimizer == \"de\":\n","                res = optimizer.DifferentialEvol(kernelFunc, {\"theta\":RealKernels(self.kernel.bounds)}, \n","                                        n_iter=self.n_restarts_optimizer * 20 if self.n_restarts_optimizer > 0 else 20,\n","                                        verbose=0).optim()\n","            elif self.optimizer == \"hs\":\n","                res = optimizer.HarmonySearch(kernelFunc, {\"theta\":RealKernels(self.kernel.bounds)}, \n","                                        n_iter=self.n_restarts_optimizer * 20 if self.n_restarts_optimizer > 0 else 20,\n","                                        verbose=0).optim()\n","            elif self.optimizer == \"pso\":\n","                res = optimizer.ParticleSwarm(kernelFunc, {\"theta\":RealKernels(self.kernel.bounds)}, \n","                                        n_iter=self.n_restarts_optimizer * 20 if self.n_restarts_optimizer > 0 else 20,\n","                                        verbose=0).optim()\n","                # res = Optimizer().optim(kernelFunc, {\"theta\":RealKernels(self.kernel.bounds)}, self.optimizer, \n","                #                         n_iter=self.n_restarts_optimizer * 20 if self.n_restarts_optimizer > 0 else 20,\n","                #                         verbose=0)\n","                # print(\"Res : \", res)\n","            self.kernel.theta = res.x\n","                # res.plot_fitness()\n","\n","\n","        K = self.kernel(self.X_train)\n","        K[np.diag_indices_from(K)] += self.epsilon\n","        self.L = cholesky(K)\n","        self.alpha = cholesky_solve(self.L, self.y_train)\n","\n","        return self\n","    \n","    def predict(self, X, return_std:bool=False, return_cov:bool=False):\n","        if self.X_train is None:\n","            self.y_mean = np.zeros(X.shape[0])\n","            if return_cov and return_std:\n","                y_cov = self.kernel(X)\n","                return self.y_mean, y_cov, np.diag(y_cov)\n","            elif return_cov:\n","                y_cov = self.kernel(X)\n","                return self.y_mean, y_cov\n","            elif return_std:\n","                y_var = np.diag(self.kernel(X))\n","                return self.y_mean, np.sqrt(y_var)\n","            else:\n","                return self.y_mean\n","        else:\n","            K_ = self.kernel(X, self.X_train)\n","            y_mean = K_ @ self.alpha\n","            y_mean = self._y_train_std * y_mean + self._y_train_mean\n","\n","        V = forwardSubstituition(self.L, K_.T)\n","        if return_cov:\n","            y_cov = self.kernel(X) - V.T @ V\n","            y_cov = np.outer(y_cov, self._y_train_std ** 2).reshape(*y_cov.shape, -1)\n","            if y_cov.shape[2] == 1:\n","                y_cov = np.squeeze(y_cov, axis=2)\n","            return y_mean, y_cov\n","        elif return_std:\n","            y_var = self.kernel.diag(X)\n","            y_var -= np.einsum(\"ij,ji->i\", V.T, V)\n","            y_var = np.outer(y_var, self._y_train_std ** 2).reshape(*y_var.shape, -1)\n","            if y_var.shape[1] == 1:\n","                y_var = np.squeeze(y_var, axis=1)\n","            return y_mean, np.sqrt(y_var)\n","        else:\n","            return y_mean\n","\n","\n","    def sample_y(self, X, n_samples=1, random_state=0):\n","        rng = np.random.RandomState(random_state)\n","        y_mean, y_cov = self.predict(X, return_cov=True)\n","        y_samples = rng.multivariate_normal(y_mean.reshape(y_mean.shape[0]), y_cov, n_samples).T\n","        return y_samples\n","    \n","    def score(self, X, y):\n","        y_pred = self.predict(X)\n","        return r2_score(y, y_pred)\n","    \n","    def log_marginal_likelihood(self, theta, evaluate_grad):\n","        kernel = self.kernel\n","        kernel.theta = theta\n","        if evaluate_grad:\n","            K, K_gradient = kernel(self.X_train, eval_gradient=True)\n","        else:\n","            K = kernel(self.X_train)\n","\n","        K[np.diag_indices_from(K)] += self.epsilon\n","\n","        L = cholesky(K)\n","        if self.y_train.ndim == 1:\n","            y_train = self.y_train[..., np.newaxis]\n","        else:\n","            y_train = self.y_train\n","\n","        alpha = cholesky_solve(L, y_train)\n","        \n","        log_likelihood_dims = -0.5 * np.einsum(\"ik, ik->k\", y_train, alpha)\n","        log_likelihood_dims -= np.log(np.diag(L)).sum()\n","        log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)\n","\n","        log_likelihood = log_likelihood_dims.sum(axis=-1)\n","\n","        if evaluate_grad:\n","            # Compute outer product\n","            # Equivalent to the following lines:\n","            # for idx in range(alpha.shape[0]):\n","            #     for jdx in range(alpha.shape[0]):\n","            #         E[idx, jdx, :] = alpha[idx, :] * alpha[jdx, :]\n","            inner_term = np.einsum(\"ik,jk->ijk\", alpha, alpha)\n","            K_inv = cholesky_solve(L, np.eye(K.shape[0]))\n","            inner_term -= K_inv[..., np.newaxis]\n","            # Compute trace\n","            log_likelihood_gradient_dims = 0.5 * np.einsum(\n","                \"ijl,jik->kl\", inner_term, K_gradient\n","            )\n","            log_likelihood_gradient = log_likelihood_gradient_dims.sum(axis=-1)\n","\n","            return log_likelihood, log_likelihood_gradient\n","        else:\n","            return log_likelihood\n","\n","\n","        \n","    def optimization(self, obj_func, initial_theta, bounds, optimizer):\n","        if optimizer == \"l-bfgs-b\":\n","            opt_res = scipy.optimize.minimize(\n","                obj_func,\n","                initial_theta,\n","                method=\"L-BFGS-B\",\n","                jac=True,\n","                bounds=bounds,\n","            )\n","\n","        return opt_res.x, opt_res.fun"]},{"cell_type":"markdown","metadata":{},"source":["# Implement Bayesian Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-11-30T01:52:17.84654Z","iopub.status.busy":"2023-11-30T01:52:17.84626Z","iopub.status.idle":"2023-11-30T01:52:17.858359Z","shell.execute_reply":"2023-11-30T01:52:17.85746Z","shell.execute_reply.started":"2023-11-30T01:52:17.846517Z"},"trusted":true},"outputs":[],"source":["%%writefile bayesOpt.py\n","\n","import acquisition as acqfunc, numpy as np\n","from GPR import GPR\n","from scipy.optimize import minimize\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","import pandas as pd\n","from paramsSpace import convert_to_bound, convert_to_params, AcqParams, decode_acq_params\n","from tqdm import tqdm\n","import optimizer\n","\n","class BayesianOptimizer:\n","    def __init__(self, f:callable=None, params=None, init_X=None, init_y=None, n_iter=10, n_init=5, \n","                kernel=None, verbose=0, random_state=None,\n","                **gp_params) -> None:\n","        self.f = f\n","        self.__n_iter = n_iter\n","        self.init = n_init\n","        self.X_train = init_X\n","        self.y_train = init_y\n","        self.acq_history = []\n","        self.params = params\n","        self.random_state = random_state\n","        self.__verbose = verbose\n","        self.best_params:dict\n","        self.best_score = None\n","\n","        if self.random_state is None:\n","            self.random_state = np.random.RandomState()\n","        elif isinstance(random_state, int):\n","            self.random_state = np.random.RandomState(random_state)\n","        else:\n","            assert isinstance(random_state, np.random.RandomState)\n","\n","        if self.X_train is None and self.y_train is None and f is not None:\n","            self.X_train = np.zeros((n_init, len(params)))\n","            for idx, p in enumerate(params):\n","                self.X_train[:, idx] = params[p].sample(shape=n_init)\n","\n","            self.y_train = np.zeros((n_init, ))\n","\n","            iteration = range(self.X_train.shape[0])\n","            if verbose:\n","                iteration = tqdm(iteration, desc=f\"Initializing Bayesian Opt\")\n","            for idx in iteration:\n","                self.y_train[idx] = f(**(convert_to_params(self.X_train[idx], self.params)))                \n","            if self.y_train.ndim == 1:\n","                self.y_train = self.y_train.reshape(-1, 1)\n","\n","        self.pbounds = convert_to_bound(params)\n","\n","        # Set the default acquisition function\n","        self.acqFunc = lambda x: acqfunc.AcqFunc(gp=self.gp).func(x.reshape(1, -1), \"ucb\", kappa=2.576)\n","        self.gp = GPR(kernel=kernel, random_state=self.random_state, **gp_params)\n","\n","    def set_func(self, f:callable, params:dict=None):\n","        if params is not None:\n","            self.params = params\n","            self.pbounds = convert_to_bound(params)     \n","        else:\n","            params = self.params\n","\n","        if self.X_train is None and self.y_train is None:\n","            self.X_train = np.zeros((self.init, len(params)))\n","            for idx, p in enumerate(params):\n","                self.X_train[:, idx] = params[p].sample(shape=self.init)\n","\n","            self.y_train = np.zeros((self.init, ))\n","            iteration = range(self.X_train.shape[0])\n","            if self.__verbose:\n","                iteration = tqdm(iteration, desc=f\"Initializing Bayesian Opt\")\n","            for idx in iteration:\n","                self.y_train[idx] = f(**(convert_to_params(self.X_train[idx], self.params)))                \n","            if self.y_train.ndim == 1:\n","                self.y_train = self.y_train.reshape(-1, 1)\n","        self.f = f\n","\n","    def set_params(self, params:dict):\n","        self.params = params\n","\n","    def set_acqfunc(self, acqfunction:[\"ucb\", \"lcb\", \"pi\", \"ei\"]=\"ucb\", **acq_params):\n","        '''\n","        Set acquisition function and pass in the parameters of acquisition function, for\n","        detail, refer to AcqFunc class\n","        '''\n","        self.__acq_type = acqfunction\n","        self.__acq_params = acq_params\n","\n","    def optimize(self, opt=\"de\",  n_samples=25, **opt_params):\n","        if (self.f is None or self.params is None):\n","            raise ValueError(\"Please ensure you provide target function and the search space\")\n","        \n","        params = {\n","            \"x\":AcqParams(self.params)\n","        }\n","\n","        iteration = range(self.__n_iter)\n","        if self.__verbose:\n","            iteration = tqdm(iteration, desc=\"Bayesian Optimization\")\n","\n","        for idx in iteration:\n","            if self.__verbose:\n","                iteration.set_postfix_str(f\"best_f : {round(self.y_train.max(), 5)}\")\n","            max_acq = -np.inf\n","            self.gp = self.gp.fit(self.X_train, self.y_train)\n","            self.acqFunc = lambda x: acqfunc.AcqFunc(gp=self.gp).func(x.reshape(1, -1), self.__acq_type, y_opt=self.y_train.max(), **self.__acq_params)\n","                        \n","            if opt == \"de\":\n","                res = optimizer.DifferentialEvol(self.acqFunc, params, \n","                                        popsize=n_samples,\n","                                        random_state=self.random_state,\n","                                        **opt_params).optim()\n","            elif opt == \"hs\":\n","                res = optimizer.HarmonySearch(self.acqFunc, params,\n","                                        popsize=n_samples, \n","                                        random_state=self.random_state,\n","                                        **opt_params).optim()\n","            elif opt == \"pso\":\n","                res = optimizer.ParticleSwarm(self.acqFunc, params, \n","                                        popsize=n_samples,\n","                                        random_state=self.random_state,\n","                                        **opt_params).optim()\n","            x_max = res.x\n","            x_max_encoded = res.x_decoded\n","            max_acq = res.fun\n","\n","            self.acq_history.append(max_acq.squeeze() if type(max_acq) == np.array else max_acq)\n","            self.X_train = np.vstack((self.X_train, [x_max]))\n","            self.y_train = np.vstack((self.y_train, [self.f(**decode_acq_params(x_max_encoded, self.params))]))\n","\n","        best_idx = np.argmax(self.y_train)\n","        self.best_params = convert_to_params(self.X_train[best_idx], self.params)\n","        self.best_score = np.max(self.y_train)\n","\n","    def res(self) -> pd.DataFrame:\n","        acq = [\"/\"] * self.init\n","        acq.extend(self.acq_history)\n","        decoded = []\n","        for x in self.X_train:\n","            decoded.append(convert_to_params(x, self.params))\n","        df = pd.DataFrame(\n","            {\n","                \"Acq\":acq,\n","                \"func\":self.y_train.flatten(),\n","                **{k:[p[k] for p in decoded] for k in self.params},\n","            }\n","        )\n","        return df"]},{"cell_type":"markdown","metadata":{},"source":["# Implement train, validation and test generator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.860357Z","iopub.status.busy":"2023-11-30T01:52:17.860053Z","iopub.status.idle":"2023-11-30T01:52:17.869341Z","shell.execute_reply":"2023-11-30T01:52:17.868539Z","shell.execute_reply.started":"2023-11-30T01:52:17.860333Z"},"trusted":true},"outputs":[],"source":["def initiateGenerator(path, batchSize, imageSize):\n","    base_path = path\n","    print(\"\\nTotal : \", end=\" \")\n","    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(batch_size=batchSize, \n","                                                                        directory=base_path+\"/\"+\"train\")\n","\n","    train_datagen = ImageDataGenerator(rescale=1./255)\n","\n","    print(\"\\nFor Training : \", end=\" \")\n","    train_generator = train_datagen.flow_from_directory(\n","        base_path+\"/\"+\"train\",\n","        target_size=(imageSize, imageSize),\n","        batch_size=batchSize,\n","        class_mode='categorical', subset='training')\n","\n","    print(\"\\nFor Val : \", end=\" \")\n","    valid_datagen = ImageDataGenerator(rescale=1./255)\n","    validation_generator = valid_datagen.flow_from_directory(\n","        base_path+\"/\"+\"valid\",\n","#                 base_path+\"/\"+\"train\",\n","\n","#         base_path + \"/\" + \"Training\",\n","        target_size=(imageSize, imageSize),\n","        batch_size=batchSize,\n","        class_mode='categorical',shuffle=False)\n","    \n","    print(\"\\nFor Test : \", end=\" \")\n","    test_datagen = ImageDataGenerator(rescale=1./255)\n","    test_generator = test_datagen.flow_from_directory(\n","#         base_path+\"/\"+\"Testing\",\n","        base_path + \"/\" + \"test\",\n","        target_size=(imageSize, imageSize),\n","        batch_size=batchSize,\n","        class_mode='categorical', shuffle=False)\n","    class_names = train_dataset.class_names\n","    noOfClasses = len(class_names)\n","    print(\"\\nNo of Classes : \", noOfClasses)\n","    print(\"Classes : \", class_names)\n","\n","        \n","    return noOfClasses,class_names, train_generator, validation_generator, test_generator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:17.870947Z","iopub.status.busy":"2023-11-30T01:52:17.870685Z","iopub.status.idle":"2023-11-30T01:52:33.987816Z","shell.execute_reply":"2023-11-30T01:52:33.986695Z","shell.execute_reply.started":"2023-11-30T01:52:17.870924Z"},"trusted":true},"outputs":[],"source":["from paramsSpace import Int, Real, Cat\n","from bayesOpt import BayesianOptimizer\n","import optimizer\n","from keras import backend as K\n","from sklearn.gaussian_process import kernels\n","\n","import tensorflow as tf\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras import layers, models\n","from keras.datasets import cifar10\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Rescaling, Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n","import numpy as np\n","from sklearn.utils import class_weight\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def plot(model, model_name, labels, X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    y_pred = np.argmax(y_pred, axis=1)\n","\n","    recall = recall_score(y_test, y_pred,average='weighted')\n","    p = precision_score(y_test, y_pred,average='weighted')\n","    f1 = f1_score(y_test, y_pred,average='weighted')\n","    acc = accuracy_score(y_test, y_pred)\n","\n","    conf_mat = confusion_matrix(y_test, y_pred)\n","    print(conf_mat)\n","    conf_df = pd.DataFrame(conf_mat, index=labels, columns=labels)\n","    plt.figure(figsize=(10, 8))\n","    plt.title(f\"{model_name}_{acc:.3f}\")\n","    sns.heatmap(conf_df, annot=True, fmt=\"g\")\n","    plt.savefig( f\"{model_name}_{acc:.3f}_confusionMatrix.png\")\n","    plt.show()\n","\n","    print(classification_report(y_test, y_pred))\n","    report = {\n","        c : 0 for c in labels\n","    }\n","    report.update(classification_report(y_test, y_pred, output_dict=True))\n","    for idx, _ in enumerate(labels):\n","        report[_] = report[f\"{idx}\"]\n","        del report[f\"{idx}\"]\n","    del report[\"accuracy\"]\n","    df = pd.DataFrame(report).transpose()\n","    plt.figure(figsize=(10, 8))\n","    plt.title(f\"{model_name}_{acc:.3f}\")\n","    sns.heatmap(df, annot=True)\n","    plt.savefig(f\"{model_name}_{acc:.3f}_classificationReport.png\")\n","    plt.show()\n","\n","mpath = \"/kaggle/input/birds-20-species-image-classification\"\n","imageSize = 64\n","\n","# Change batch_size and epochs\n","batchSize = 16\n","epochs = 30\n","\n","noOfClasses, class_names, train_generator, validation_generator, test_generator = initiateGenerator(mpath, batchSize=batchSize, imageSize=imageSize)\n","INPUT_SHAPE = (imageSize, imageSize, 3)\n","KERNEL_SIZE = (3, 3)\n","\n","# Calculate class_weight to deal with classes imbalance problem\n","class_weight = class_weight.compute_class_weight(\n","                class_weight='balanced',\n","                classes=np.unique(train_generator.classes), \n","                y=train_generator.classes)\n","class_weight = {x : class_weight[x] for x in range(len(class_weight))}\n","print(\"class weight: \", class_weight)"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Base Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T01:52:33.990022Z","iopub.status.busy":"2023-11-30T01:52:33.989672Z","iopub.status.idle":"2023-11-30T01:56:39.02776Z","shell.execute_reply":"2023-11-30T01:56:39.026208Z","shell.execute_reply.started":"2023-11-30T01:52:33.989991Z"},"trusted":true},"outputs":[],"source":["record = []\n","y_best = 0\n","\n","for idx in range(100):\n","\n","    model = Sequential()\n","\n","    # Change Layer1\n","    model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    # Change Layer2\n","    model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    \n","    # Change Drop1\n","    model.add(Dropout(0.25))\n","\n","    # Change Layer3\n","    model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    # Change Layer4\n","    model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    \n","    # Change Drop2\n","    model.add(Dropout(0.25))\n","\n","    # Change Layer5\n","    model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    # Change Layer6\n","    model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    \n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    \n","    # Change Drop3\n","    model.add(Dropout(0.25))\n","\n","    model.add(Flatten())\n","\n","    # Change Layer7\n","    model.add(Dense(128, activation='relu'))\n","    \n","    # Change Drop4\n","    model.add(Dropout(0.25))\n","    \n","    model.add(Dense(noOfClasses, activation='softmax'))\n","\n","    # Change optimizer\n","    model.compile(optimizer='adam',\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy']\n","                )\n","    \n","    # Set learning_rate\n","    K.set_value(model.optimizer.learning_rate, 0.001)\n","\n","    # Train the model\n","    model.fit(train_generator, epochs=epochs, \n","            validation_data=validation_generator,\n","            class_weight=class_weight\n","            )\n","    test_loss, test_acc = model.evaluate(test_generator)\n","    record.append(test_acc)\n","    if test_acc > y_best:\n","        y_best = test_acc\n","        \n","        # Change the \"Naive\" to the category you choose\n","        plot(model, \"Naive\", class_names, test_generator, test_generator.classes)\n","        \n","# Change the csv name\n","pd.DataFrame({\"acc\":record}).to_csv(f\"Naive.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Blackbox Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, verbose=1, min_lr=1e-5, mode=\"max\")\n","early = EarlyStopping(monitor=\"val_accuracy\", patience=3, verbose=1, mode=\"max\", baseline=0.2)\n","\n","\n","def hyperTune(y_best=None, filename=\"\", **kwargs):\n","    train_generator.batch_size = kwargs[\"batch_size\"]\n","    validation_generator.batch_size = kwargs[\"batch_size\"]\n","    test_generator.batch_size = kwargs[\"batch_size\"]\n","\n","    model = Sequential()\n","    model.add(Conv2D(filters=kwargs[\"Layer1_filter\"], kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation=kwargs[\"Layer1_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(filters=kwargs[\"Layer2_filter\"], kernel_size=KERNEL_SIZE, activation=kwargs[\"Layer2_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(kwargs[\"Drop1\"]))\n","    \n","    model.add(Conv2D(filters=kwargs[\"Layer3_filter\"], kernel_size=KERNEL_SIZE, activation=kwargs[\"Layer3_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(filters=kwargs[\"Layer4_filter\"], kernel_size=KERNEL_SIZE, activation=kwargs[\"Layer4_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(kwargs[\"Drop2\"]))\n","\n","    model.add(Conv2D(filters=kwargs[\"Layer5_filter\"], kernel_size=KERNEL_SIZE, activation=kwargs[\"Layer5_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(filters=kwargs[\"Layer6_filter\"], kernel_size=KERNEL_SIZE, activation=kwargs[\"Layer6_act\"], padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(kwargs[\"Drop3\"]))\n","\n","    model.add(Flatten())\n","    model.add(Dense(kwargs[\"Layer7_units\"], activation=kwargs[\"Layer7_act\"]))\n","    model.add(Dropout(kwargs[\"Drop4\"]))\n","    model.add(Dense(noOfClasses, activation='softmax'))\n","\n","    # Compile the model\n","    model.compile(optimizer=kwargs[\"optimizer\"],\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","\n","    K.set_value(model.optimizer.learning_rate, kwargs[\"learning_rate\"])\n","    print(\n","        \"optim : \", kwargs[\"optimizer\"], \n","        \"lr : \", kwargs[\"learning_rate\"], \n","        \"bs : \", kwargs[\"batch_size\"]\n","        )\n","    print(\"optim : \", model.optimizer)\n","    print(\"lr : \", model.optimizer.learning_rate)\n","    print(\"bs : \", train_generator.batch_size)\n","\n","    model.summary()\n","\n","    # Train the model\n","    model.fit(train_generator, epochs=kwargs[\"epochs\"], \n","            validation_data=validation_generator,\n","            class_weight=class_weight,\n","            callbacks=[annealer, early])\n","    test_loss, test_acc = model.evaluate(test_generator)\n","    if y_best is not None and test_acc > y_best:\n","        plot(model, f\"{filename}_Tuned\", class_names, test_generator, test_generator.classes)\n","    return test_acc"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Bayesian Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optim = BayesianOptimizer(\n","    kernel=kernels.Matern(nu=2.5),\n","    params=params,\n","    n_iter=100,\n","    n_init=10, \n","    n_restarts_optimizer=5,\n","    epsilon=1e-6,\n","    normalize_y=True,\n","    optimizer=\"de\",\n","    verbose=1\n",")\n","optim.set_func(lambda **x:hyperTune(optimizer.best_score(), \"byopt de\", **x))\n","optim.set_acqfunc(acqfunction=\"mix\")\n","optim.optimize(n_samples=30, \n","                opt=\"de\",\n","                n_iter=10,\n","                )\n","print(optim.res())\n","optim.res().to_csv(\"byopt de tuned.csv\")\n","\n","optim = BayesianOptimizer(\n","    kernel=kernels.Matern(nu=2.5),\n","    params=params,\n","    n_iter=100,\n","    n_init=10, \n","    n_restarts_optimizer=5,\n","    epsilon=1e-6,\n","    normalize_y=True,\n","    optimizer=\"pso\",\n","    verbose=1\n",")\n","optim.set_func(lambda **x:hyperTune(optim.best_score(), \"byopt pso\", **x))\n","optim.set_acqfunc(acqfunction=\"mix\")\n","optim.optimize(n_samples=30, \n","                opt=\"pso\",\n","                n_iter=10,\n","                )\n","print(optim.res())\n","optim.res().to_csv(\"byopt pso tuned.csv\")\n","\n","optim = BayesianOptimizer(\n","    kernel=kernels.Matern(nu=2.5),\n","    params=params,\n","    n_iter=100,\n","    n_init=10, \n","    n_restarts_optimizer=5,\n","    epsilon=1e-6,\n","    normalize_y=True,\n","    optimizer=\"hs\",\n","    verbose=1\n",")\n","optim.set_func(lambda **x:hyperTune(optim.best_score(), \"byopt hs\", **x))\n","optim.set_acqfunc(acqfunction=\"mix\")\n","optim.optimize(n_samples=30, \n","                opt=\"hs\",\n","                n_iter=10,\n","                )\n","print(optim.res())\n","optim.res().to_csv(\"byopt hs tuned.csv\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Optimzier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import optimizer\n","\n","de_optimizer = optimizer.DifferentialEvol(params=params, popsize=10, n_iter=10, verbose=1, \n","#                                           random_state=42\n","                                        )\n","de_optimizer.set_func(lambda **x:hyperTune(de_optimizer.best_score, \"DE\", **x))\n","res = de_optimizer.optim()\n","print(res.res())\n","res.res().to_csv(\"de tuned.csv\")\n","print(de_optimizer.best_params)\n","print(de_optimizer.best_score)\n","\n","pso_optimizer = optimizer.ParticleSwarm(params=params, popsize=10, n_iter=10, verbose=1, \n","#                                           random_state=42\n","                                        )\n","pso_optimizer.set_func(lambda **x:hyperTune(pso_optimizer.best_score, \"PSO\", **x))\n","res = pso_optimizer.optim()\n","print(res.res())\n","res.res().to_csv(\"pso tuned.csv\")\n","print(pso_optimizer.best_params)\n","print(pso_optimizer.best_score)\n","\n","hs_optimizer = optimizer.HarmonySearch(params=params, popsize=10, n_iter=10, verbose=1, \n","#                                           random_state=42\n","                                        )\n","hs_optimizer.set_func(lambda **x:hyperTune(hs_optimizer.best_score, \"HS\", **x))\n","res = hs_optimizer.optim()\n","print(res.res())\n","res.res().to_csv(\"hs tuned.csv\")\n","print(hs_optimizer.best_params)\n","print(hs_optimizer.best_score)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":672377,"sourceId":1183165,"sourceType":"datasetVersion"},{"datasetId":3027308,"sourceId":5205289,"sourceType":"datasetVersion"},{"datasetId":3093062,"sourceId":5324005,"sourceType":"datasetVersion"},{"datasetId":534640,"sourceId":5468571,"sourceType":"datasetVersion"},{"datasetId":3356546,"sourceId":5838737,"sourceType":"datasetVersion"},{"datasetId":4024114,"sourceId":7000210,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
